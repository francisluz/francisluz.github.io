{"pageProps":{"post":{"title":"How to integrate container security scan with AWS Elastic Search and Kibana","date":"2020-08-05T22:02:28.274Z","slug":"aws-elastic-search-aqua-security","author":{"name":"Francis Luz","picture":"/assets/blog/authors/francis.jpg"},"content":"<h2>AWS Elastic Search with Cognito (for Kibana), Lambda and Aqua Security (kube-bench) Reports</h2>\n<h1>Using Cloudformation templates to manange the cloud stack</h1>\n<p><a \n  href=\"https://github.com/francisluz/aws-elasticsearch-cognito-aquasec\" \n  target=\"_blank\" \n  class=\"mx-2 font-bold\">\n<img src=\"https://cdn.jsdelivr.net/npm/simple-icons@3.0.1/icons/github.svg\" \n    alt=\"GitHub Repo\" height=\"22px\" width=\"22px\" class=\"inline mr-2 fill-current\"/>\nGitHub Repo\n</a></p>\n<img src=\"https://raw.githubusercontent.com/francisluz/aws-elasticsearch-cognito-aquasec/master/images/AWS-ES-Kibana-Diagram.png\" alt=\"AWS Elastic Search with Cognito and Kibana Diagram\">\n<p>The idea of this project is to put together all resources needed to create a Elastic Search cluster with Kibana as visualition tool. </p>\n<p>Which will ingest data reports from Aqua Security kube-bench a Container Security monitor.</p>\n<h1>Table of Contents</h1>\n<ul>\n<li><a href=\"#prerequisites\">Prerequisites</a><ul>\n<li><a href=\"#execution-rights\">Execution rights</a></li>\n</ul></li>\n<li><a href=\"#aws-elastic-search\">AWS Elastic Search</a><ul>\n<li>Components</li>\n<li>Deployments</li>\n<li>Cluster for Production</li>\n<li>Execution</li>\n</ul></li>\n<li><a href=\"#kibana-with-cognito\">Kibana with Cognito</a><ul>\n<li>Cognito</li>\n</ul></li>\n<li><a href=\"#lambda-with-s3\">Lambda with S3</a><ul>\n<li>Components</li>\n<li>Deployments</li>\n<li>Execution</li>\n</ul></li>\n<li><a href=\"#aws-eks-cluster\">AWS EKS Cluster</a></li>\n<li><a href=\"#integration-test\">Integration Test</a></li>\n</ul>\n<h2>Prerequisites</h2>\n<p>We will need the basic <a href=\"https://aws.amazon.com/cli/\"><strong>AWS CLI</strong></a> setup.</p>\n<p>If you don't have a AWS Account here's good guide: <a href=\"https://serverless.com/framework/docs/providers/aws/guide/credentials/\">Creating AWS Credetials</a>.</p>\n<p>Basic commands to setup your AWS CLI.</p>\n<pre><code class=\"hljs language-bash\">pip install awscli\naws --version\naws configure\naws iam get-user\n\n<span class=\"hljs-comment\"># Check Elastic Search versions to see which one suits you best</span>\naws es list-elasticsearch-versions\naws es list-elasticsearch-instance-types --elasticsearch-version 7.1</code></pre>\n<h3>Execution rights</h3>\n<p>To be able to run the <strong><code>.sh</code></strong> deployment files you may need to give them rights.</p>\n<p>Example:</p>\n<pre><code class=\"hljs language-sh\">chmod +x ./deploy.sh</code></pre>\n<h2>AWS Elastic Search</h2>\n<p>The launch of AWS Elastic Search cluster, involves a set of resources needed in order to acomplish the main goal which is release the cluster with security.</p>\n<h3>Components</h3>\n<p>All the components needed are setup in the cloudformation template <strong><code>aws-es-domain.yml</code></strong>.</p>\n<ul>\n<li>IAM Roles</li>\n<li>Cognito User Pool</li>\n<li>Congnito Identity Pool</li>\n<li>Elastic Search Domain</li>\n</ul>\n<h3>Deployment</h3>\n<p>Before you run provide your unique values by changing the parameters at <strong><code>deploy.sh</code></strong>: </p>\n<ul>\n<li>&#x3C;GLOBAL_UNIQUE_DOMAIN_NAME></li>\n<li>&#x3C;UNIQUE_POOL_NAME></li>\n<li>&#x3C;GLOBAL_UNIQUE_POOL_DOMAIN></li>\n</ul>\n<h3>Cluster for Production</h3>\n<p>If your aim is to deploy the cluster in production or simulate a more powerfull one you need to uncomment the lines in the section <strong><code>ElasticsearchDomain</code></strong> at:</p>\n<ul>\n<li>aws-es-domain.yml</li>\n</ul>\n<pre><code class=\"hljs language-yml\"><span class=\"hljs-attr\">ZoneAwarenessEnabled:</span> <span class=\"hljs-string\">\"true\"</span>\n<span class=\"hljs-attr\">DedicatedMasterEnabled:</span> <span class=\"hljs-string\">\"true\"</span>\n<span class=\"hljs-attr\">DedicatedMasterType:</span> <span class=\"hljs-string\">\"t2.small.elasticsearch\"</span>\n<span class=\"hljs-attr\">DedicatedMasterCount:</span> <span class=\"hljs-string\">\"1\"</span></code></pre>\n<h3>Execution</h3>\n<p>The deployment can take from 10 to 15 minutes to finish.</p>\n<p><strong>Run</strong></p>\n<pre><code class=\"hljs language-sh\">./deploy.sh</code></pre>\n<h2>Kibana with Cognito</h2>\n<p>The Kibana setup is verify straight forward now that you have deploy the main Elastic Search script.</p>\n<p>Create an user in your User Pool by accessing the AWS Console.</p>\n<h3>Cognito</h3>\n<ul>\n<li>Manager User Pools</li>\n<li>Your pool<ul>\n<li><strong>Users and groups</strong> \\\nSetup the User name and Password.</li>\n</ul></li>\n</ul>\n<h2>Lambda with S3</h2>\n<p>The Lambda function is the code that will stream the data to Elastic Search via <strong>S3 Trigger</strong>.</p>\n<h3>Components</h3>\n<ul>\n<li>AWS S3</li>\n<li>IAM Role</li>\n<li>Lambda Function (Python 3.7)</li>\n</ul>\n<h3>Deployment</h3>\n<p>Fist of all execute the <strong><code>deploy-bucket.sh</code></strong> to create the bucket base for the lambda.</p>\n<p>Update the <strong>lambda function</strong> with the new Elastic Search host name at <strong><code>lambda/s3-to-es.py</code></strong>.</p>\n<p>After the execution of the bucket deployment it will gerenate the <strong><code>bucket name</code></strong> that you will need to execute the next step.</p>\n<p>Now you have to update the parameters with the value from the previous step at <strong><code>deploy-lambda.sh</code></strong> and also inform the name of your <strong>ingestion bucket</strong>:</p>\n<ul>\n<li>--s3-bucket &#x3C;YOUR_BUCKET_NAME></li>\n<li>&#x3C;INGESTION_BUCKET_NAME></li>\n</ul>\n<h3>Execution</h3>\n<p>This process will create the <strong>insgestion bucket</strong>,\npackage, deploy the lambda and create the trigger event between S3 and the function.</p>\n<p><strong>Run</strong></p>\n<pre><code class=\"hljs language-sh\">./deploy-lambda.sh</code></pre>\n<h2>AWS EKS Cluster</h2>\n<p>This step is optional if you already have it setup in your environment, and I also provide a sample file at <strong><code>aquasec-log/kube-bench-report.json</code></strong> that you can test the ingestion workflow without having to launche a new cluster.</p>\n<p>For this section you can follow the steps defined on <a href=\"https://github.com/aquasecurity/kube-bench#running-in-an-eks-cluster\"><strong>kube-bench</strong></a> repository.</p>\n<p><strong>Notice</strong>: In the file <code>job-eks.yml</code> at the <strong>command</strong> line add the <code>--json</code> flag to generate the correct output.</p>\n<pre><code class=\"hljs language-sh\"><span class=\"hljs-built_in\">command</span>: [<span class=\"hljs-string\">\"kube-bench\"</span>, <span class=\"hljs-string\">\"--version\"</span>, <span class=\"hljs-string\">\"1.11\"</span>, <span class=\"hljs-string\">\"--json\"</span>]</code></pre>\n<p>Based on the main repo guide you can find a sample deployment script here:</p>\n<ul>\n<li><strong><code>deploy-aws-eks.sh</code></strong></li>\n</ul>\n<h2>Integration Test</h2>\n<p>To complete the integration test create a folder <strong><code>load</code></strong> inside of you <strong>ingestion bucket</strong> and then <strong>upload</strong> the sample file:</p>\n<ul>\n<li><strong><code>aquasec-log/kube-bench-report.json</code></strong></li>\n</ul>\n<p>After the upload it should trigger the lambda and create a new index on Elastic Search.</p>\n<p>Access your <strong>Kibana</strong> and check your new index there.</p>\n<h2>Cloud Cleanup</h2>\n<p>As we are using Cloudformation which track the changesets, you're able to rollback and delete any resource used. Go to the AWS Console and <strong>Cloud Formation</strong>.</p>\n<p>#aws #aws-es #aws-eks #aws-lambda #aws-s3 #shell-script #python</p>\n","ogImage":{"url":"/assets/blog/aws-elastic-search-aqua-security/cover.jpg"},"coverImage":"/assets/blog/aws-elastic-search-aqua-security/cover.jpg"}},"__N_SSG":true}